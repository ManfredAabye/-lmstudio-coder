# LM Studio KI Coder

Dieses Tool aktualisiert automatisch Quellcode-Dateien in einem Verzeichnis mithilfe einer lokalen KI (z.‚ÄØB. LM Studio + LLM). Unterst√ºtzt werden viele Programmiersprachen wie Python, C++, JavaScript, Rust u.‚ÄØv.‚ÄØm.

## ‚ú® Funktionen

- Lokale Verarbeitung √ºber LM Studio (kein Cloud-Zwang)
- Unterst√ºtzung f√ºr zahlreiche Programmiersprachen
- Backups aller bearbeiteten Dateien
- Individuell anpassbarer Prompt

---

## üß† Voraussetzungen

- [LM Studio](https://lmstudio.ai) installiert und gestartet
- Ein passendes LLM-Modell ist geladen und als **lokaler Chat-Endpunkt** erreichbar:

[http://localhost:1234/v1/chat/completions](http://localhost:1234/v1/chat/completions)

> Tipp: In LM Studio unter ‚ÄûAPI‚Äú den lokalen Server aktivieren und Port 1234 verwenden.

---

## üõ† Installation

1. **Python 3.8+** installieren (falls noch nicht vorhanden)
2. Erforderliche Pakete (nur Standard-Libraries werden genutzt)
3. Repository klonen oder Dateien lokal speichern
4. Programm starten:

 ```bash
 python KI-Code-Updater.py
````

---

## üöÄ Verwendung

1. **Arbeitsverzeichnis** w√§hlen (z.‚ÄØB. ein Projektordner mit `.py`, `.js` usw.)
2. **Programmiersprache** ausw√§hlen
3. **Prompt anpassen** (optional)

   * Platzhalter `{language}` und `{code}` werden automatisch ersetzt
4. Auf **‚ÄûCode aktualisieren‚Äú** klicken
5. Ergebnis abwarten ‚Äì Fortschrittsbalken zeigt den Status an

> ‚úÖ Alle bearbeiteten Dateien werden vorher gesichert (Ordner `backups/` im jeweiligen Quellverzeichnis).

---

## üîß Beispiel-Prompt

```text
Aktualisiere diesen {language}-Code:
- Ersetze veraltete oder unsichere Syntax
- Behalte die Funktionalit√§t bei

Code:
{code}
```

---

## üìÅ Unterst√ºtzte Sprachen & Dateiendungen

| Sprache    | Endung  |
| ---------- | ------- |
| Assembly  | .asm    |
| Bash      | .sh     |
| Batch     | .bat    |
| C#        | .cs     |
| C++       | .cpp    |
| CSS       | .css    |
| F#        | .fs     |
| Go        | .go     |
| HTML      | .html   |
| JSON      | .json   |
| Java      | .java   |
| JavaScript| .js     |
| Lua       | .lua    |
| Markdown  | .md     |
| PHP       | .php    |
| PowerShell| .ps1    |
| Python    | .py     |
| Ruby      | .rb     |
| Rust      | .rs     |
| SQL       | .sql    |
| Shell     | .sh     |
| TypeScript| .ts     |
| VBScript  | .vbs    |
| Visual Basic| .vb   |
| XML       | .xml    |

> Die vollst√§ndige Liste wird dynamisch aus dem internen Mapping erzeugt.

---

## ‚ùì Fehlersuche

* **‚ÄûAPI-Fehler: Verbindung abgelehnt‚Äú** ‚Üí Ist LM Studio aktiv und auf Port 1234?
* **Keine Dateien gefunden** ‚Üí Verzeichnis korrekt gew√§hlt? Sprache und Dateiendung stimmen √ºberein?

---

Hier ist die optimale Konfiguration f√ºr **Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf** in LM Studio, um die besten Ergebnisse mit Ihrem Code-Updater zu erzielen:

## 1. **Modell in LM Studio laden**
- Herunterladen der Modelldatei ([z.B. von HuggingFace](https://huggingface.co/TheBloke/Meta-Llama-3-8B-Instruct-GGUF))
- In LM Studio: 
  - Zu **"Models"** navigieren
  - Modelldatei (`Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`) ausw√§hlen

## 2. **Empfohlene Modelleinstellungen**
| Einstellung          | Optimaler Wert       | Erkl√§rung |
|----------------------|----------------------|-----------|
| **Context Length**   | 4096                 | Maximale Kontextl√§nge f√ºr Codeanalyse |
| **Temperature**      | 0.3 - 0.5            | F√ºr pr√§zise Code-Updates (nicht zu kreativ) |
| **Top-K**           | 40                   | Balance zwischen Qualit√§t und Vielfalt |
| **Top-P**           | 0.9                  | Filtert unwahrscheinliche Optionen |
| **Repeat Penalty**  | 1.1                  | Vermeidet Wiederholungen im Code |

## 3. **API-Server starten**
1. In LM Studio zu **"Local Server"** wechseln
2. Folgende API-Einstellungen w√§hlen:
   ```json
   {
     "port": 1234,
     "enable_api": true,
     "model": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
     "context_length": 4096
   }
   ```
3. **"Start Server"** klicken

## 4. **Code-Updater anpassen (optional)**
In `file_processor.py` die API-Parameter optimieren:
```python
response = requests.post(
    "http://localhost:1234/v1/chat/completions",
    json={
        "messages": [
            {"role": "system", "content": f"Du bist ein {language}-Experte. Antworte NUR mit Code."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,  # F√ºr konservative Code-√Ñnderungen
        "top_k": 40,
        "top_p": 0.9,
        "max_tokens": 4000
    },
    timeout=180
)
```

## 5. **Prompt-Engineering f√ºr bessere Ergebnisse**
```text
Aktualisiere diesen {language}-Code:
- Behalte die Funktionalit√§t bei
- Ersetze veraltete/deprecated Funktionen
- Verbessere die Lesbarkeit
- F√ºge kurze Kommentare hinzu wo n√∂tig

Code:
{code}
```

## üí° Tipps f√ºr beste Performance:
1. **GPU-Beschleunigung aktivieren** (falls verf√ºgbar)
2. **Nicht zu viele Dateien parallel** verarbeiten (LM Studio arbeitet besser sequenziell)
3. **Chunk-Gr√∂√üe** in `file_processor.py` auf ~6000 Zeichen begrenzen

Mit diesen Einstellungen erhalten Sie:
- Pr√§zisere Code-Updates
- Bessere Beibehaltung der Originalfunktionalit√§t
- Schnellere Verarbeitung durch optimierte Parameter
